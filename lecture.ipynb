{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from urllib import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이썬으로 크롤링하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip > pypi > pipenv > library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyenv 가상환경 설정\n",
    "\n",
    "pyenv virtualenv 3.6.5 <가상환경명>\n",
    "\n",
    "pyenv local <가상환경명>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests 모듈\n",
    "\n",
    "파이썬으로 HTTP get 요청을 하기 위한 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://comic.naver.com/webtoon/weekday.nhn'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:10.0) Gecko/20100101 Firefox/10.0'\n",
    "}\n",
    "res = requests.get(url, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('weekday.html', 'wt')\n",
    "# f.write(res.text)\n",
    "# f.close()\n",
    "\n",
    "# with open('weekday.html', 'wt') as f:\n",
    "#     f.write(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_doc = \"\"\"\n",
    "# <html><head><title>The Dormouse's story</title></head>\n",
    "# <body>\n",
    "# <p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "# <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "# <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "# <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "# <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "# and they lived at the bottom of a well.</p>\n",
    "\n",
    "# <p class=\"story\">...</p>\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = open('weekday.html', 'rt').read()\n",
    "# soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# div_content = soup.find('div', id='content')\n",
    "# div_list_area = div_content.find('div', class_='list_area')\n",
    "# div_col_list = div_list_area.find_all('div', class_='col')\n",
    "\n",
    "# for col in div_col_list:\n",
    "#     print(col.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'weekday.html'\n",
    "# url_episode_list = 'https://comic.naver.com/webtoon/list.nhn'\n",
    "# params = {\n",
    "#     'titleId': 703845,\n",
    "# }\n",
    "# if os.path.exists(file_path):\n",
    "#     html = open(file_path, 'rt').read()\n",
    "# else:\n",
    "#     response = requests.get(url_episode_list, params)\n",
    "#     html = response.text\n",
    "#     open(file_path, 'wt').write(html)\n",
    "\n",
    "# soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# h2_title = soup.select_one('div.detail > h2')\n",
    "# title = h2_title.contents[0].strip()\n",
    "# author = h2_title.contents[1].get_text(strip=True)\n",
    "# # desc = soup.select_one('div.detail > h2').next_sibling.next_element\n",
    "# desc = soup.select_one('div.detail > p').get_text()\n",
    "\n",
    "# print(desc)\n",
    "# print(title)\n",
    "# print(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = soup.select_one('table.viewList')\n",
    "\n",
    "\n",
    "# tr_list = table.select('tr')\n",
    "\n",
    "# for index, tr in enumerate(tr_list[1:]):\n",
    "#     if tr.get('class'):\n",
    "#         continue\n",
    "        \n",
    "#     url_thumnail = tr.select_one('td:nth-of-type(1) img').get('src')\n",
    "#     url_detail = tr.select_one('td > a').get('href')\n",
    "    \n",
    "#     query_string = parse.urlsplit(url_detail).query\n",
    "#     query_dict = parse.parse_qs(query_string)\n",
    "    \n",
    "#     no = query_dict['no'][0]\n",
    "    \n",
    "#     title = tr.select_one('td:nth-of-type(2) > a').get_text(strip=True)\n",
    "#     rating = tr.select_one('td:nth-of-type(3) strong').get_text(strip=True)\n",
    "#     created_data = tr.select_one('td:nth-of-type(4)').get_text(strip=True)\n",
    "    \n",
    "# #     print(detail)\n",
    "#     print(no)\n",
    "# #     print(url_thumnail)\n",
    "# #     print(url_detail)\n",
    "# #     print(title)\n",
    "# #     print(rating)\n",
    "# #     print(created_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Webtoon:\n",
    "    def __init__(self, webtoon_id):\n",
    "        self.webtoon_id = webtoon_id\n",
    "        info = self.get_info()\n",
    "        self.title = info['title']\n",
    "        self.author = info['author']\n",
    "        self.desc = info['desc']\n",
    "\n",
    "        \n",
    "    def get_html(self):\n",
    "        file_path = 'data/episode_list-{webtoon_id}.html'.format(webtoon_id=self.webtoon_id)\n",
    "        url_episode_list = 'https://comic.naver.com/webtoon/list.nhn'\n",
    "        params = {\n",
    "            'titleId': self.webtoon_id,\n",
    "        }\n",
    "        if os.path.exists(file_path):\n",
    "            html = open(file_path, 'rt').read()\n",
    "\n",
    "        else:\n",
    "            response = requests.get(url_episode_list, params)\n",
    "            html = response.text\n",
    "            open(file_path, 'wt').write(html)\n",
    "            \n",
    "        return html\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"\n",
    "        webtoon_id 매개변수를 이용하여\n",
    "        웹툰 title, author, description을 딕셔너리 형태로 return\n",
    "        params webtoon_id:\n",
    "        return: title, author, description 딕셔너리로\n",
    "        \"\"\"\n",
    "        html = self.get_html()\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        h2_title = soup.select_one('div.detail > h2')\n",
    "        title = h2_title.contents[0].strip()\n",
    "        author = h2_title.contents[1].get_text(strip=True)\n",
    "        desc = soup.select_one('div.detail > p').get_text()\n",
    "\n",
    "        webtoon_dict = {\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'desc': desc,\n",
    "        }\n",
    "\n",
    "        return webtoon_dict\n",
    "\n",
    "    def episode_crawler(self):\n",
    "        \n",
    "        html = self.get_html()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        table = soup.select_one('table.viewList')\n",
    "\n",
    "        tr_list = table.select('tr')\n",
    "\n",
    "        for index, tr in enumerate(tr_list[1:]):\n",
    "            if tr.get('class'):\n",
    "                continue\n",
    "\n",
    "            url_thumnail = tr.select_one('td:nth-of-type(1) img').get('src')\n",
    "            url_detail = tr.select_one('td > a').get('href')\n",
    "\n",
    "            query_string = parse.urlsplit(url_detail).query\n",
    "            query_dict = parse.parse_qs(query_string)\n",
    "\n",
    "            no = query_dict['no'][0]\n",
    "\n",
    "            title = tr.select_one('td:nth-of-type(2) > a').get_text(strip=True)\n",
    "            rating = tr.select_one('td:nth-of-type(3) strong').get_text(strip=True)\n",
    "            created_data = tr.select_one('td:nth-of-type(4)').get_text(strip=True)\n",
    "\n",
    "        epsisode_dict = {\n",
    "            'title': title,\n",
    "            'rating': rating,\n",
    "            'desc': desc,\n",
    "        }\n",
    "        \n",
    "        return episode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Webtoon(718019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'옆집친구'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10minutes_to_pandas.ipynb\r\n",
      "20170528204207_6e9df8e618b97520233bbb35e7d4eaaf_IMAG01_1.jpg\r\n",
      "20170528204207_6e9df8e618b97520233bbb35e7d4eaaf_IMAG01_2.jpg\r\n",
      "20170528204207_6e9df8e618b97520233bbb35e7d4eaaf_IMAG01_3.jpg\r\n",
      "Pipfile\r\n",
      "Pipfile.lock\r\n",
      "Titanic_kaggle.ipynb\r\n",
      "bugs.ipynb\r\n",
      "bugs_top_100.csv\r\n",
      "bugs_top_100.xlsx\r\n",
      "crawling_theroy.ipynb\r\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m/\r\n",
      "kaggle_titanic_train.csv\r\n",
      "lecture.ipynb\r\n",
      "weekday.html\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5655653]\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    'titleId': [234234], 'no': [5655653]\n",
    "}\n",
    "\n",
    "print(query['no'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_one 과 select 의 차이\n",
    "\n",
    "- select_one: 요소 하나를 가져옴\n",
    "\n",
    "- select: 요소의 배열을 모두 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단순하게 생각해서 list 는 get_text( ) 를 사용할 수 없다고 생각하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래에 members 라는 변수에 list 를 만들었다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list 안에 값들을 꺼내오는 가장 일반적인 방법이 무엇일까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = ['민수', '영희', '준희']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for 문을 돌려서 값들을 하나씩 출력하는 방법이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "민수\n",
      "<class 'str'>\n",
      "영희\n",
      "<class 'str'>\n",
      "준희\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for member in members:\n",
    "    print(member)\n",
    "    print(type(member))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그렇기 때문에 데이터타입 `list` 속성은 get_text()를 사용할 수 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select_one('a').get_text() # a 태그 하나에 대해서만 text 값을 추출하기 때문에 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = soup.select('a')\n",
    "\n",
    "for menu in a_list:\n",
    "    print(menu.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그 선택자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select_one('span')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아이디 선택자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select_one('#where')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스 선택자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('.blind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
